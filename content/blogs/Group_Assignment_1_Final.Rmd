---
categories:
- ""
- ""
date: "2017-10-31T22:42:51-05:00"
description: Nullam et orci eu lorem consequat tincidunt vivamus et sagittis magna
  sed nunc rhoncus condimentum sem. In efficitur ligula tate urna. Maecenas massa
  sed magna lacinia magna pellentesque lorem ipsum dolor. Nullam et orci eu lorem
  consequat tincidunt. Vivamus et sagittis tempus.
draft: false
image: franki-chamaki-1K6IQsQbizI-unsplash.jpg
keywords: ""
slug: max
title: LBS work 1
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2,dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
library(rvest) # to scrape wikipedia page
```

# Where Do People Drink The Most Beer, Wine And Spirits?

Back in 2014, [fivethiryeight.com](https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/) published an article on alchohol consumption in different countries. The data `drinks` is available as part of the `fivethirtyeight` package. Make sure you have installed the `fivethirtyeight` package before proceeding.

```{r load_alcohol_data}
library(fivethirtyeight)
data(drinks)

# or download directly
#alcohol_direct <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv")

```

### What are the variable types? Any missing values we should worry about?

By using the skim function, we discovered that the variable **country** is of character type, while the other four variables are of numeric type.

The skim function also confirmed that no values are missing in the dataset.

```{r glimpse_skim_data}
skim(drinks)
```

### Make a plot that shows the top 25 beer consuming countries

```{r beer_plot}
library(ggplot2)
drinks %>%
  slice_max(order_by = beer_servings, n=25) %>%
  ggplot(aes(x=beer_servings, y=fct_reorder(country,beer_servings)))+
  geom_col(fill='blue')+
  theme_bw()+
  labs(
    title = "Top 25 beer consuming countries",
    subtitle = "",
    x = "Cans of beer consumed per person",
    y = "Country"
  )+

  NULL
```

### Make a plot that shows the top 25 wine consuming countries

```{r wine_plot}
drinks %>%
  slice_max(order_by = wine_servings, n=25) %>%
  ggplot(aes(x=wine_servings, y=fct_reorder(country,wine_servings)))+
  geom_col(fill='green')+
  theme_bw()+
  labs(
    title = "Top 25 wine consuming countries",
    subtitle = "",
    x = "Glasses of wine consumed per person",
    y = "Country"
  )+

  NULL

```

### Finally, make a plot that shows the top 25 spirit consuming countries

```{r spirit_plot}
drinks %>%
  slice_max(order_by = spirit_servings, n=25) %>%
  ggplot(aes(x=spirit_servings, y=fct_reorder(country,spirit_servings)))+
  geom_col(fill='red')+
  theme_bw()+
  labs(
    title = "Top 25 spirit consuming countries",
    subtitle = "",
    x = "Shots of spirit consumed per person",
    y = "Country"
  )+

  NULL

```

### What can you infer from these plots? Don't just explain what's in the graph, but speculate or tell a short story (1-2 paragraphs max).


From the plots above, we can infer how heavily alcohol is being consumed in each country broken up into beer, wine and spirits.  It is certainly no surprise that people in Czech are among the most beer-consuming countries, but Namibia is a little surprising with about 370 yearly beer cans consumed per person. That implies every person drinks on average about 1 beer per day. However, Namibia is a German colony and therefore has adopted the German Beer drinking traditions. A little surprising is the fact that Germany didn't make it into the Top 3, but Gabon has. Regarding wine consumption, it is no surprise that people in France are the heaviest consumers having several world famous wineries. The average consumption is about 370 glasses of wine per person per year. Last but not least, Russia and Belarus are no surprise to be placed in the Top 3 spirit consumers with their popularity for drinking vodka, with  about 370 and 320 shots of spirit consumed per person, respectively. A bit more interesting is Grenada leading the table, with approximately 430 servings per person, but a Caribbean country like itself probably has the right environment for a lot of rum drinking on the beach.\n
  
In general, we can see that more than half of the top 25 beer and wine drinking countries come from Europe which are countries that tend to be more developed and have a higher GDP. In comparison, top spirit drinking countries tend to be more developing countries such as Grenada and Haiti. This may be explained by the fact that wine and beer are more expensive to purchase than spirits thus people in developing countries opt for cheaper alternatives when it comes to drinking. Furthermore, such national or continental differences in alcohol tastes could also be explained by the national availability of the ingredients to make this alcohol. For France, there has been enormous productions of grapes every year for manufacturing wine. For Germany, barley has been one of their main crops, which is then used for beer fermenting. For Japan and China, rice, their main crop, has been widely used to produce spirits. Moreover, these differences in tastes have also been preserved by many local traditions, customs and national culture.\n 
  
# Analysis of movies- IMDB dataset

We will look at a subset sample of movies, taken from the [Kaggle IMDB 5000 movie dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset)

```{r load_movies, warning=FALSE, message=FALSE}

movies <- read_csv(here::here("data","movies.csv"))
glimpse(movies)

```

Besides the obvious variables of `title`, `genre`, `director`, `year`, and `duration`, the rest of the variables are as follows:

-   `gross` : The gross earnings in the US box office, not adjusted for inflation
-   `budget`: The movie's budget
-   `cast_facebook_likes`: the number of facebook likes cast memebrs received
-   `votes`: the number of people who voted for (or rated) the movie in IMDB
-   `reviews`: the number of reviews for that movie
-   `rating`: IMDB average rating

## Use your data import, inspection, and cleaning skills to answer the following:

-   Are there any missing values (NAs)? Are all entries distinct or are there **duplicate entries**?

There are no missing values, but several duplicate entries. The dataframe has **2961** rows, even though only **2907** unique movie entries are present. Consequently, **54** entries are duplicates.\n 
  
It might be possible that movies have the same name, but by doing some random checks, such as on the duplicate "Spider-Man 3", we were able to detect that in fact duplicate rows exist.\n 
  

```{r skim movie entries}
skim(movies)

#Identifying duplicate values
movies[duplicated(movies$title), ]

#Looking at random duplicate
movies %>%
  filter(title=="Spider-Man 3")
```

-   Produce a table with the count of movies by genre, ranked in descending order

```{r number of movies by genre }
movies %>%
  group_by(genre)%>%
  summarise(count=n())%>%
  arrange(desc(count))


```

-   Produce a table with the average gross earning and budget (`gross` and `budget`) by genre. Calculate a variable `return_on_budget` which shows how many \$ did a movie make at the box office for each \$ of its budget. Ranked genres by this `return_on_budget` in descending order

```{r average gross earning and budget by genre}
movies %>%
  group_by(genre)%>%
  summarise(average_gross_earnings = mean(gross), average_budget = mean(budget))

```

```{r ranking genres by average return on budget}
movies_budget_return <- movies %>%
  mutate(return_on_budget = (gross / budget))

movies_budget_return %>%
  group_by(genre) %>%
  summarize(average_return_on_budget = mean(return_on_budget))%>%
  arrange(desc(average_return_on_budget))

```

-   Produce a table that shows the top 15 directors who have created the highest gross revenue in the box office. Don't just show the total gross amount, but also the mean, median, and standard deviation per director.

```{r top 15 directors}
movies_top15_directors <- movies%>%
  group_by(director) %>%
  summarise(total_gross_director = sum(gross),
            mean_director = mean(gross),
            median_director = median(gross),
            sd_director = sd(gross))%>%
  slice_max(total_gross_director, n=15)%>%
  knitr::kable(bootstrap_options = c ("striped","hover","condensed","responsive"))

movies_top15_directors

```

-   Finally, ratings. Produce a table that describes how ratings are distributed by genre. We don't want just the mean, but also, min, max, median, SD and some kind of a histogram or density graph that visually shows how ratings are distributed.

```{r rating distribution by genre}
movies_ratings_by_genre <- movies %>%
  group_by(genre) %>%
  summarize(
    mean_rating = mean(rating),
    min_rating = min(rating),
    max_rating = max(rating),
    median_rating = median(rating),
    sd_rating = sd(rating)
    )

movies_ratings_by_genre

ggplot(movies, aes(x=rating))+
  geom_density()+
  theme_bw()+
  labs(
    title = "Distribution of Ratings by Genre",
    subtitle = "",
    x = "Movie Ratings",
    y = "Density")+
  facet_wrap(~genre)+
  NULL
```

## Use `ggplot` to answer the following

-   Examine the relationship between `gross` and `cast_facebook_likes`. Produce a scatterplot and write one sentence discussing whether the number of facebook likes that the cast has received is likely to be a good predictor of how much money a movie will make at the box office. What variable are you going to map to the Y- and X- axes?

```{r gross_on_fblikes}
ggplot(movies, aes(x=cast_facebook_likes, y=gross))+
  geom_point(alpha=0.3)+
  geom_smooth(method = "lm")+
  theme_bw()+
  labs(
    title="Correlation between Gross Income and Cast Facebook likes",
    x="Cast Facebook Likes",
    y="Gross Income"
    )+
  scale_y_log10()+ #To counteract the skew, we implemented log to make it 
  #visually clearer
  scale_x_log10()+
  NULL
```

The graph shows some positive correlation between the facebook likes of the cast and the gross income of the movie. Even though the correlation is weak, facebook likes can be used as an indicator for how much money the movie will make, but many likes are still no guarantee for success.\n

-   Examine the relationship between `gross` and `budget`. Produce a scatterplot and write one sentence discussing whether budget is likely to be a good predictor of how much money a movie will make at the box office.

```{r gross_on_budget}
ggplot(movies, aes(x=budget, y=gross))+
  geom_point(alpha=0.3)+
  geom_smooth(method = "lm")+
  theme_bw()+
  labs(
    title="Correlation between Gross Income and Movie Budget",
    x="Budget",
    y="Gross"
  )+
  scale_y_log10()+
  scale_x_log10()+
  NULL
```

The graph displays a strong positive correlation between movie budget and the gross income, seen by the steepness of the line. Therefore, movie budget is a good indicator of how much money the movie will earn.\n

-   Examine the relationship between `gross` and `rating`. Produce a scatterplot, faceted by `genre` and discuss whether IMDB ratings are likely to be a good predictor of how much money a movie will make at the box office. Is there anything strange in this dataset?

```{r gross_on_rating}
ggplot(movies, aes(x=rating, y=gross))+
  geom_point(alpha=0.3)+
  geom_smooth(method = "lm")+
  theme_bw()+
  labs(
    title="Correlation between IMDB Rating and Gross Income by genre",
    x="IMDB Rating",
    y="Gross Income"
  )+
  scale_y_log10()+
  facet_wrap(~genre)
  NULL
```

Generally, it can be seen that IMDB Rating is not a good indicator for predicting gross income depending on the genre. The fact that only very few romances and thrillers are part of the dataset is remarkable, as usually many movies should fall into the genres, but it seems like they are rather allocated to categories like "Drama", "Action", "Comedy" or "Adventure". It is clear that for many genres like Western, Musical and Family we only have a few entries so we cannot determine a relationship with certainty.\n 

# Returns of financial stocks

> You may find useful the material on [finance data sources](https://mam2022.netlify.app/reference/finance_data/).

We will use the `tidyquant` package to download historical data of stock prices, calculate returns, and examine the distribution of returns.

We must first identify which stocks we want to download data for, and for this we must know their ticker symbol; Apple is known as AAPL, Microsoft as MSFT, McDonald's as MCD, etc. The file `nyse.csv` contains 508 stocks listed on the NYSE, their ticker `symbol`, `name`, the IPO (Initial Public Offering) year, and the sector and industry the company is in.

```{r load_nyse_data, message=FALSE, warning=FALSE}
nyse <- read_csv(here::here("data","nyse.csv"))
```

Based on this dataset, create a table and a bar plot that shows the number of companies per sector, in descending order

```{r companies_per_sector}

nyse_sector_count <- nyse %>%
  group_by(sector) %>%
  summarize(
    count=n()
  )%>%
  arrange(desc(count))

nyse_sector_count

ggplot(nyse, aes(x=sector))+
  geom_bar()+
  theme_bw()+
  labs(
    title="Number of Companies per Sector",
    x="Sector",
    y="Count"
  )+ 
  scale_x_discrete(limits = nyse_sector_count$sector)+
  theme(axis.text.x = element_text(angle=90, vjust=0.5, hjust=1))
  NULL
```

Next, let's choose some stocks and their ticker symbols and download some data. You **MUST** choose 6 different stocks from the ones listed below; You should, however, add `SPY` which is the SP500 ETF (Exchange Traded Fund).

```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}
# Notice the cache=TRUE argument in the chunk options. Because getting data is time consuming, 
# cache=TRUE means that once it downloads data, the chunk will not run again next time you knit your Rmd

myStocks <- c("JPM","DIS","DPZ","ANF","TSLA","XOM","SPY" ) %>%
  tq_get(get  = "stock.prices",
         from = "2011-01-01",
         to   = "2021-08-31") %>%
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame
```

Financial performance analysis depend on returns; If I buy a stock today for 100 and I sell it tomorrow for 101.75, my one-day return, assuming no transaction costs, is 1.75%. So given the adjusted closing prices, our first step is to calculate daily and monthly returns.

```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```

Create a table where you summarise monthly returns for each of the stocks and `SPY`; min, max, median, mean, SD.

```{r summarise_monthly_returns}

myStocks_returns_monthly_bystock <- myStocks_returns_monthly %>%
  group_by(symbol)%>%
  summarize(
    min_return = min(monthly_returns),
    max_return = max(monthly_returns),
    median_return = median(monthly_returns),
    mean_return = mean(monthly_returns),
    sd_return = sd(monthly_returns)
  )

myStocks_returns_monthly_bystock

```

Plot a density plot, using `geom_density()`, for each of the stocks

```{r density_monthly_returns}

ggplot(myStocks_returns_monthly, aes(x=monthly_returns))+
  geom_density()+
  theme_bw()+
  facet_wrap(~symbol)+
  labs(
    title="Density Plot for Monthly Returns",
    x="Monthly Returns",
    y="Density"
  )+
  NULL

```

What can you infer from this plot? Which stock is the riskiest? The least risky?

The riskiest stock is **TSLA** as it has a high volatility, which implies large potential gains, but also large potential losses. On the other hand, **SPY** is relatively stable around zero with slightly positive returns. It has a low volatility with low upside but also low downside, implying it is quite stable and a risk-averse investment opportunity.\n 

Finally, make a plot that shows the expected monthly return (mean) of a stock on the Y axis and the risk (standard deviation) in the X-axis. Please use `ggrepel::geom_text_repel()` to label each stock

```{r risk_return_plot}

ggplot(myStocks_returns_monthly_bystock, aes(x=sd_return, y=mean_return,
                                             label=symbol))+
  geom_point()+
  theme_bw()+
  geom_smooth(method = "lm")+
  ggrepel::geom_text_repel()+
  labs(
    title="Correlation between Risk and Return (Monthly)",
    x="Risk(Standard Deviation)",
    y="Average Returns"
  )+
    NULL

```

What can you infer from this plot? Are there any stocks which, while being riskier, do not have a higher expected return?

Generally, there is a relationship between higher risk and higher return, except for stock **ANF**. This stock has an extremely high risk while having a relatively low return.\n 

# On your own: IBM HR Analytics

For this task, you will analyse a data set on Human Resource Analytics. The [IBM HR Analytics Employee Attrition & Performance data set](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset) is a fictional data set created by IBM data scientists. Among other things, the data set includes employees' income, their distance from work, their position in the company, their level of education, etc. A full description can be found on the website.

First let us load the data

```{r load HR data}

hr_dataset <- read_csv(here::here("data","datasets_1067_1925_WA_Fn-UseC_-HR-Employee-Attrition.csv"))
glimpse(hr_dataset)

```

I am going to clean the data set, as variable names are in capital letters, some variables are not really necessary, and some variables, e.g., `education` are given as a number rather than a more useful description

```{r clean variables}

hr_cleaned <- hr_dataset %>% 
  clean_names() %>% 
  mutate(
    education = case_when(
      education == 1 ~ "Below College",
      education == 2 ~ "College",
      education == 3 ~ "Bachelor",
      education == 4 ~ "Master",
      education == 5 ~ "Doctor"
    ),
    environment_satisfaction = case_when(
      environment_satisfaction == 1 ~ "Low",
      environment_satisfaction == 2 ~ "Medium",
      environment_satisfaction == 3 ~ "High",
      environment_satisfaction == 4 ~ "Very High"
    ),
    job_satisfaction = case_when(
      job_satisfaction == 1 ~ "Low",
      job_satisfaction == 2 ~ "Medium",
      job_satisfaction == 3 ~ "High",
      job_satisfaction == 4 ~ "Very High"
    ),
    performance_rating = case_when(
      performance_rating == 1 ~ "Low",
      performance_rating == 2 ~ "Good",
      performance_rating == 3 ~ "Excellent",
      performance_rating == 4 ~ "Outstanding"
    ),
    work_life_balance = case_when(
      work_life_balance == 1 ~ "Bad",
      work_life_balance == 2 ~ "Good",
      work_life_balance == 3 ~ "Better",
      work_life_balance == 4 ~ "Best"
    )
  ) %>% 
  select(age, attrition, daily_rate, department,
         distance_from_home, education,
         gender, job_role,environment_satisfaction,
         job_satisfaction, marital_status,
         monthly_income, num_companies_worked, percent_salary_hike,
         performance_rating, total_working_years,
         work_life_balance, years_at_company,
         years_since_last_promotion)

```

Produce a one-page summary describing this data set. Here is a non-exhaustive list of questions:

The IBM Human Resources data provides a lot of information on IBM employees such as personal details, incomes and satisfaction. By drawing out different tables, graphs and using summary statistics based on the data we can arrive at valuable insights.\n

Firstly, we begin by looking at attrition rates in IBM. In Table 1 we can see that the company has an attrition rate of 16.1%. This means that 16.1 percent of the company's listed employees left the company at the time the data set was created, which is not that high considering many young people may be using IBM as an opportunity to learn and then move into more senior positions at different companies. This can further be seen by drawing out a box plot comparing 'attrition' and the total number of years worked (Figure 1). From the box plots it is evident that it's the people with less years of work that tend to leave the company. This however is only a hypothesis and needs some further analysis in order to be concluded with certainty.\n 

Looking more closely at the distributions of some of the variables we can compare the mean and median in combination with 1st and 3rd quadrant, to make some inferences about normality and skewness. When the mean and median differ too much, there is a skewness and the distribution of the given variable is not normal. For example, age seems to be fairly normally distributed, even though the 1st and 3rd quadrant are not equally far away from the mean. Years at the company, years since last promotion and monthly income have some extreme outliers and differing median and mean making normality improbable. To confirm this, we plot density plots for each variable. It is clear that the variable that is closest to a normal distribution is 'Age' with the other three variables being heavily right skewed. From this, we can infer that employees across all ages can be found within IBM. At the same time there are extremely highly paid employees skewing the monthly income distribution and it is also evident that most employees are in the company for 0-10 years with only a few staying for longer periods of time. \n

An important area to explore is that of job satisfaction and work life balance in IBM. We can conclude that the majority of IBM employees seem to be happy with their job as from tables 3 and 4 we see that about 61.3% of employees report to have a Very High/High job satisfaction and a surprising 94.6% have at least a 'Good' work life balance. Only very few (5.4%) report a bad work life balance, however, 20% of employees are not satisfied with their job. This may be due to a number of different reasons such as income levels or hours worked, but it is always expected that some people will not be happy at their work and thus this percentage is not of major concern.\n

Next, we investigate how income levels compare to education. The box plots (figure 6) and histograms for education with monthly income (figure 10) indicate that more educated people tend to receive higher salaries. There are some outliers in all categories, except the 'Doctor' category. This indicates that we can find people from all academic backgrounds in senior, highly paid positions. Moreover, the distributions seem to be mostly skewed for the majority of education levels especially for employees with a Bachelors degree. Furthermore, plotting a bar chart (figure 9) of average incomes with education the conclusion that more educated people receive higher incomes is further supported. It is worth noting, however, that there is a big increase in average income for employees holding a 'Doctorate' where as for the rest of the education levels the increase in incomes is more gradual as education increases.\n

When looking at the box plots for monthly income with gender, (figure 7) interestingly enough, it seems that incomes are distributed quite similarly across genders with similar spread of values and medians of approximately 5000 dollars. This means that there is no gender income gap in IBM. Female employees even have a higher 25th and 75th percentile and slightly higher median, implying they receive slightly higher salaries.This is a very positive find as it shows that modern companies like IBM are promoting gender equality. \n

It is clear that income levels vary greatly between Job roles. From the box plots in figure 8 we can see that the highest paid positions are the 'Managers' and 'Research Directors' with respective medians of about 17 500 and 16 000 dollars. The spread of incomes for these two roles appear much higher than the rest. The job with the lowest pay is that of a 'Sales Representative' with income levels being very tightly spread around 2000-3000 dollars. 

Lastly, we wanted to investigate how income varies with age across different job roles. From the plots in figure 11, we can see that for many jobs employees seem to be getting similar incomes across all ages with slight increases as people get older for some positions. For positions such as Healthcare representatives and Research Directors this phenomenon is more evident. This is quite interesting as usually we expect that older people are associated with more experience and thus higher pay but this is not the case for all positions here. We should note however that for the highest paid jobs of 'Manager' and 'Research Director' there are no 20-30 year olds employed so in general it is older people that work in the highest paid positions.
  

1.  How often do people leave the company (`attrition`)

**Table 1**
```{r people leaving company}
hr_attrition_summarized <- hr_cleaned %>%
  count(attrition)

hr_attrition_summarized$percentage <- hr_attrition_summarized$n/sum(hr_attrition_summarized$n)

hr_attrition_summarized

```

```{r attrition vs. years worked}
ggplot(hr_cleaned,aes(x=attrition,y=total_working_years))+
                        geom_boxplot()+
  labs(
    title="Box plot for attrition and years worked",
    subtitle= "Figure 1",
    x="Attrition",
    y="Total years worked"
  )
         NULL
```

2.  How are `age`, `years_at_company`, `monthly_income` and `years_since_last_promotion` distributed? can you roughly guess which of these variables is closer to Normal just by looking at summary statistics?

**Table 2**
```{r summary statistics}
hr_cleaned %>%
  select(c("age", "years_at_company", "monthly_income", 
           "years_since_last_promotion"))%>%
  summary()
```


```{r Distribution of IBM variables}
hr_cleaned %>%
  ggplot(aes(x=age))+
  geom_density()+
  theme_bw()+
  labs(
    title="Density Plot for Age",
    subtitle= "Figure 2",
    x="Age",
    y="Density"
  )+
  NULL

hr_cleaned %>%
  ggplot(aes(x=years_at_company))+
  geom_density()+
  theme_bw()+
  labs(
    title="Density Plot for Years at the company",
    subtitle= "Figure 3",
    x="Years at the company",
    y="Density"
  )+
  NULL

hr_cleaned %>%
  ggplot(aes(x=monthly_income))+
  geom_density()+
  theme_bw()+
  labs(
    title="Density Plot for Monthly income",
    subtitle= "Figure 4",
    x="Monthly Income",
    y="Density"
  )+
  NULL

hr_cleaned %>%
  ggplot(aes(x=years_since_last_promotion))+
  geom_density()+
  theme_bw()+
  labs(
    title="Density Plot for Years since last promotion",
    subtitle= "Figure 5",
    x="Years since last promotion",
    y="Density"
  )+
  NULL
```


3.  How are `job_satisfaction` and `work_life_balance` distributed? Don't just report counts, but express categories as % of total

**Table 3**
```{r distribution of job satisfaction}
hr_jobsatisfaction <- hr_cleaned %>%
  count(job_satisfaction)
hr_jobsatisfaction$percentage <- (hr_jobsatisfaction$n/sum(hr_jobsatisfaction$n)*100)
hr_jobsatisfaction
```

**Table 4**
```{r distribution of work life balance}
hr_worklbal <- hr_cleaned %>%
  count(work_life_balance)
hr_worklbal$percentage <- (hr_worklbal$n/sum(hr_worklbal$n))*100
hr_worklbal
```


4.  Is there any relationship between monthly income and education? Monthly income and gender?

**Table 5**
```{r relationship monthly income and education}
hr_income_education <- hr_cleaned %>%
  group_by(education) %>%
  summarize(
    avg_income = mean(monthly_income)
  )

hr_income_education

ggplot(hr_cleaned, aes(x=reorder(education,monthly_income), y=monthly_income))+
  geom_boxplot()+
  labs(
    title="Boxplot of Monthly income according to education",
    subtitle= "Figure 6",
    x="Education",
    y="Income"
  )+
  NULL
```

**Table 6**
```{r relationship between income and gender}
hr_income_gender <- hr_cleaned %>%
  group_by(gender) %>%
  summarize(
    avg_income = mean(monthly_income)
  )

hr_income_gender

ggplot(hr_cleaned, aes(x=gender,y= monthly_income))+
  geom_boxplot()+
  labs(
    title="Boxplot of Monthly income according to gender",
    subtitle= "Figure 7",
    x="Gender",
    y="Income"
  )+
  NULL
```

5.  Plot a boxplot of income vs job role. Make sure the highest-paid job roles appear first

```{r income vs. job role}
ggplot(hr_cleaned, aes(x=reorder(job_role,-monthly_income), y=monthly_income))+
         geom_boxplot()+
  labs(
    title="Job Role vs. Income",
    subtitle= "Figure 8",
    x="Job Role",
    y="Income"
  )+
  theme(axis.text.x = element_text(angle=90, vjust=0.5, hjust=1))+
  NULL
  
```

6.  Calculate and plot a bar chart of the mean (or median?) income by education level.

```{r income by education bar chart}
hr_mean_income_education <- hr_cleaned %>%
  group_by(education) %>%
  summarize(
    mean_income=mean(monthly_income),
    median_income=median(monthly_income)
    )

ggplot(hr_mean_income_education,aes(x=reorder(education, -mean_income), 
                                    y=mean_income))+
  geom_col()+
  labs(
    title="Average Income according to Education Level",
    subtitle= "Figure 9",
    x="Education Level",
    y="Average income"
    )+
  NULL
```

7.  Plot the distribution of income by education level. Use a facet_wrap and a theme from `ggthemes`

```{r distribution income by education level}
ggplot(hr_cleaned,aes(x=monthly_income))+
  geom_histogram()+
  facet_wrap(~education)+
  theme_wsj()+
  theme(plot.title = element_text(size=15),
        plot.subtitle = element_text(size=12),
        axis.text.x = element_text(angle=45, size = 10, vjust=0.5, hjust=1),
        axis.text.y = element_text(angle=45, size = 10, vjust=0.5, hjust=1))+
  labs(
    title="Distribution of Income by Education Level",
    subtitle= "Figure 10",
    x="Income",
    y="Count"
  )+

  NULL

```

8.  Plot income vs age, faceted by `job_role`

```{r income vs. age faceted by job role}
ggplot(hr_cleaned, aes(x=age, y=monthly_income))+
  geom_point(alpha=0.3)+
  geom_smooth(method = "lm")+
  theme_bw()+
  labs(
    title="Correlation between Age and Income",
    subtitle= "Figure 11",
    x="Age",
    y="Income"
  )+
  facet_wrap(~job_role)+
  NULL
  
```

# Challenge 2: Opinion polls for the 2021 German elections

The Guardian newspaper has an [election poll tracker for the upcoming German election](https://www.theguardian.com/world/2021/aug/20/german-election-poll-tracker-who-will-be-the-next-chancellor). The list of the opinion polls since Jan 2021 can be found at [Wikipedia](https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election) and your task is to reproduce the graph similar to the one produced by the Guardian.

The following code will scrape the wikipedia page and import the table in a dataframe.

```{r scrape_wikipedia_polling_data, warnings= FALSE, message=FALSE}
url <- "https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election"
# https://www.economist.com/graphic-detail/who-will-succeed-angela-merkel
# https://www.theguardian.com/world/2021/jun/21/german-election-poll-tracker-who-will-be-the-next-chancellor


# get tables that exist on wikipedia page 
tables <- url %>% 
  read_html() %>% 
  html_nodes(css="table")


# parse HTML tables into a dataframe called polls 
# Use purr::map() to create a list of all tables in URL
polls <- map(tables, . %>% 
             html_table(fill=TRUE)%>% 
             janitor::clean_names())


# list of opinion polls
german_election_polls <- polls[[1]] %>% # the first table on the page contains the list of all opinions polls
  slice(2:(n()-1)) %>%  # drop the first row, as it contains again the variable names and last row that contains 2017 results
  mutate(
         # polls are shown to run from-to, e.g. 9-13 Aug 2021. We keep the last date, 13 Aug here, as the poll date
         # and we extract it by picking the last 11 characters from that field
         end_date = str_sub(fieldwork_date, -11),
         
         # end_date is still a string, so we convert it into a date object using lubridate::dmy()
         end_date = dmy(end_date),
         
         # we also get the month and week number from the date, if we want to do analysis by month- week, etc.
         month = month(end_date),
         week = isoweek(end_date)
         )
```

```{r graph German election polls}
german_election_polls %>% select(union,spd,af_d, fdp, linke, grune, end_date)%>%
  pivot_longer(.,cols= c(union,spd,af_d,fdp,linke,grune), 
               names_to = "Political_Parties",values_to = "val") %>%
  ggplot(aes(x= end_date, y= val, fill= Political_Parties, 
             colour= Political_Parties))+
  geom_point(alpha=0.5)+
  geom_smooth(se=FALSE)+
  scale_y_continuous(labels=function(x) paste0(x,"%"))+
  scale_x_date(date_labels = "%b %y")+
  xlab("Date")+
  ylab("Percentage of votes")+
  labs(title= "German Election Polls 2021")+
  scale_color_manual(values=c("#0099FF", "#FFFF00", "#339933", "#CC0099", 
                              "#FF3300", "#000000"))+ 
  theme(plot.title=element_text(hjust=0.5))+
  knitr::opts_chunk$set(fig.width=unit(10,"cm"), fig.height=unit(10,"cm"))
```




